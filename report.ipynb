{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ruyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pandas import read_parquet\n",
    "import pandas as pd\n",
    "import preprocess.preprocess\n",
    "import preprocess.stats\n",
    "import importlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from utils import *\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Tweet Sentiment Analysis\n",
    "\n",
    "Ruyu Dai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>user_displayname</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>945</td>\n",
       "      <td>1500</td>\n",
       "      <td>1012</td>\n",
       "      <td>1012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[Bitcoin]</td>\n",
       "      <td>If the government are allowed to sell all the ...</td>\n",
       "      <td>BezosCrypto</td>\n",
       "      <td>SHIB Bezos</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         hashtags                                            content  \\\n",
       "count        1500                                               1500   \n",
       "unique        945                                               1500   \n",
       "top     [Bitcoin]  If the government are allowed to sell all the ...   \n",
       "freq          384                                                  1   \n",
       "\n",
       "           username user_displayname sentiment  \n",
       "count          1500             1500      1500  \n",
       "unique         1012             1012         2  \n",
       "top     BezosCrypto       SHIB Bezos      True  \n",
       "freq             41               41      1220  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_parquet(\"data/raw/btc_tweets_train.parquet.gzip\")\n",
    "user_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common contractions found:\n",
      "it's: 64\n",
      "don't: 44\n",
      "let's: 25\n",
      "i'm: 19\n",
      "bitcoin's: 12\n",
      "that's: 10\n",
      "what's: 9\n",
      "here's: 9\n",
      "you're: 8\n",
      "doesn't: 8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Step 1: Search for contractions\n",
    "def find_contractions(text):\n",
    "    # This regex pattern looks for words with apostrophes\n",
    "    pattern = r\"\\w+'\\w+\"\n",
    "    return re.findall(pattern, text.lower())\n",
    "\n",
    "# Collect all contractions from the dataset\n",
    "all_contractions = []\n",
    "for tweet in data['content']:\n",
    "    all_contractions.extend(find_contractions(tweet))\n",
    "\n",
    "# Count the occurrences of each contraction\n",
    "contraction_counts = Counter(all_contractions)\n",
    "\n",
    "print(\"Most common contractions found:\")\n",
    "for contraction, count in contraction_counts.most_common(10):\n",
    "    print(f\"{contraction}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This chapter is divided into three parts, which covers three general parts of the pipeline but does not follow the sequence of the pipeline. as things like bot filtering and spam filtering happens before we normalize the tweet. \n",
    "\n",
    "so each chapter is basically an illustration of whats going on for specific tweets. But for the general purpose of preprocess just run the following code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets with contractions: 283\n",
      "\n",
      "Example expansions:\n",
      "Original: Solid bid in major ALT/BTC pairs today. \n",
      "\n",
      "If #Bitcoin continues to  consolidate up here, I'm expecting +15-20% across the wider altcoin market. https://t.co/YSiClL3Aza\n",
      "Expanded: Solid bid in major ALT/BTC pairs today. \n",
      "\n",
      "If #Bitcoin continues to  consolidate up here, I am expecting +15-20% across the wider altcoin market. https://t.co/YSiClL3Aza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter tweets with contractions\n",
    "tweets_with_contractions = data[data['content'].apply(preprocess.stats.has_contraction)]\n",
    "\n",
    "# Apply expansion to filtered tweets\n",
    "tweets_with_contractions['expanded_content'] = tweets_with_contractions['content'].apply(preprocess.preprocess.expand_contractions)\n",
    "\n",
    "# Print out one example\n",
    "print(f\"Number of tweets with contractions: {len(tweets_with_contractions)}\")\n",
    "print(\"\\nExample expansions:\")\n",
    "for original, expanded in zip(tweets_with_contractions['content'].head(1), tweets_with_contractions['expanded_content'].head(10)):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Expanded: {expanded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Mention\n",
    "\n",
    "Every twitter user has a handle associated with them. Users often mention other users in their tweets by @handle. We replace all user mentions with the word USER_MENTION. The regular expression used to match user mention is `@[\\S]+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "@vorztoken @PeopleMetaverse @CGMeifangZhang @elonmusk The vision that Satoshi Nakamoto has presented through #Bitcoin is something that I deeply admire and appreciate.\n",
      "-------------------------\n",
      "After: \n",
      "    The vision that Satoshi Nakamoto has presented through #Bitcoin is something that I deeply admire and appreciate.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tweet = data['content'][1641690081470877696]\n",
    "print('Before: \\n' + tweet)\n",
    "print('-------------------------')\n",
    "print('After: \\n' + re.sub(r'@[\\S]+', '', tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL\n",
    "\n",
    "Users often share hyperlinks to other webpages in their tweets. Any particular URL is not important for text classification as it would lead to very sparse features. Therefore, we re- place all the URLs in tweets with the word URL. The regular expression used to match URLs is `((www\\.[\\S]+)|(https?://[\\S]+))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "#BitcoinÂ TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "$BTC #BitcoinÂ #BTCÂ Â Â \n",
      "\n",
      "#BitcoinÂ #BTCÂ #SHIB \n",
      "#HOGE #SAITAMA #BNBÂ Â Â #DOGE #ETH #BabyFloki #AltCoinSeason https://t.co/rtlFlKlVCv\n",
      "-------------------------\n",
      "After: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "#BitcoinÂ TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "$BTC #BitcoinÂ #BTCÂ Â Â \n",
      "\n",
      "#BitcoinÂ #BTCÂ #SHIB \n",
      "#HOGE #SAITAMA #BNBÂ Â Â #DOGE #ETH #BabyFloki #AltCoinSeason  URL \n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "tweet = data['content'].iloc[0]\n",
    "print('Before: \\n' + tweet)\n",
    "print('-------------------------')\n",
    "tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "print('After: \\n' + tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag\n",
    "\n",
    "Hashtags are unspaced phrases prefixed by the hash symbol (#) which is frequently used by users to mention a trending topic on twitter. We replace all the hashtags with the words with the hash symbol. For example, `#hello` is replaced by `hello`. The regular expression used to match hashtags is `#(\\S+)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "#BitcoinÂ TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "$BTC #BitcoinÂ #BTCÂ Â Â \n",
      "\n",
      "#BitcoinÂ #BTCÂ #SHIB \n",
      "#HOGE #SAITAMA #BNBÂ Â Â #DOGE #ETH #BabyFloki #AltCoinSeason  URL \n",
      "-------------------------\n",
      "After: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      " Bitcoin Â TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "$BTC  Bitcoin Â  BTC Â Â Â \n",
      "\n",
      " Bitcoin Â  BTC Â  SHIB  \n",
      " HOGE   SAITAMA   BNB Â Â Â  DOGE   ETH   BabyFloki   AltCoinSeason   URL \n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print('Before: \\n' + tweet)\n",
    "print('-------------------------')\n",
    "tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "print('After: \\n' + tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoji\n",
    "\n",
    "Users often use a number of different emoticons in their tweet to convey different emotions. As we are about to use vadarsentiment as a benchmark. We therefore applied the same emoji dictionary, mapping each emoji to their description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINKâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      " Bitcoin Â TO $100,000 WHETHER YOU BELIEVE OR NOTâ€¼ï¸ðŸ’¯ðŸ™\n",
      "\n",
      "$BTC  Bitcoin Â  BTC Â Â Â \n",
      "\n",
      " Bitcoin Â  BTC Â  SHIB  \n",
      " HOGE   SAITAMA   BNB Â Â Â  DOGE   ETH   BabyFloki   AltCoinSeason   URL \n",
      "-------------------------\n",
      "After: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINK double exclamation markï¸ hundred points folded hands\n",
      "\n",
      " Bitcoin Â TO $100,000 WHETHER YOU BELIEVE OR NOT double exclamation markï¸ hundred points folded hands\n",
      "\n",
      "$BTC  Bitcoin Â  BTC Â Â Â \n",
      "\n",
      " Bitcoin Â  BTC Â  SHIB  \n",
      " HOGE   SAITAMA   BNB Â Â Â  DOGE   ETH   BabyFloki   AltCoinSeason   URL\n"
     ]
    }
   ],
   "source": [
    "print('Before: \\n' + tweet)\n",
    "print('-------------------------')\n",
    "tweet = preprocess.preprocess.handle_emojis(tweet)\n",
    "print('After: \\n' + tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINK double exclamation markï¸ hundred points folded hands\n",
      "\n",
      " Bitcoin Â TO $100,000 WHETHER YOU BELIEVE OR NOT double exclamation markï¸ hundred points folded hands\n",
      "\n",
      "$BTC  Bitcoin Â  BTC Â Â Â \n",
      "\n",
      " Bitcoin Â  BTC Â  SHIB  \n",
      " HOGE   SAITAMA   BNB Â Â Â  DOGE   ETH   BabyFloki   AltCoinSeason   URL\n",
      "-------------------------\n",
      "After: \n",
      "bitcoin to     sooner than you think double exclamation mark  hundred points folded hands bitcoin to     whether you believe or not double exclamation mark  hundred points folded hands btc bitcoin btc bitcoin btc shib hoge saitama bnb doge eth babyfloki altcoinseason url\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print('Before: \\n' + tweet)\n",
    "print('-------------------------')\n",
    "tweet = preprocess.preprocess.preprocess_tweet(tweet)\n",
    "print('After: \\n' + re.sub(\"[^a-zA-Z]\",\" \", tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(preprocess.preprocess.preprocess_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "data['lemmatized_content'] = data['content'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Additional Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bot Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_sentiment, true_sentiment):\n",
    "    # Ensure both arrays are numpy arrays\n",
    "    predicted = np.array(predicted_sentiment)\n",
    "    true = np.array(true_sentiment)\n",
    "    \n",
    "    # Check if the arrays have the same shape\n",
    "    if predicted.shape != true.shape:\n",
    "        raise ValueError(\"The predicted and true sentiment arrays must have the same shape.\")\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(predicted == true)\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.87%\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vaderSentiment = []\n",
    "for sentence in data['content']:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    vaderSentiment.append(True if vs['compound']>=0 else False)\n",
    "\n",
    "accuracy = calculate_accuracy(predicted_sentiment=vaderSentiment, true_sentiment=data['sentiment'])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and Fine-tune of DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conculsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Achievements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
